# Completions

## Request

<ParamField path="prompt" type="string | array">
  The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.
  Default: `""`
</ParamField>

<ParamField path="model" type="string" required="true">
  Available options:

  * `llama3.1-8b`
  * `llama-3.3-70b`
  * `qwen-3-32b`
  * `qwen-3-235b-a22b-instruct-2507` (preview)
  * `qwen-3-235b-a22b-thinking-2507` (preview)
  * `qwen-3-coder-480b` (preview)
</ParamField>

<ParamField path="stream" type="boolean | null">
  If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.

  Default: `false`
</ParamField>

<ParamField path="return_raw_tokens" type="boolean | null">
  Return raw tokens instead of text.

  Default: `false`
</ParamField>

<ParamField path="max_tokens" type="integer | null">
  The maximum number of tokens that can be generated in the chat completion. The total length of input tokens and generated tokens is limited by the model's context length.

  Default: `null`
</ParamField>

<ParamField path="min_tokens" type="integer | null">
  The minimum number of tokens to generate for a completion. If not specified or set to 0, the model will generate as many tokens as it deems necessary. Setting to -1 sets to max sequence length.

  Default: `null`
</ParamField>

<ParamField path="grammar_root" type="string | null">
  The grammar root used for structured output generation.
  Supported values: `root`, `fcall`, `nofcall`, `insidevalue`, `value`, `object`, `array`, `string`, `number`, `funcarray`, `func`, `ws`.

  Default: `null`
</ParamField>

<ParamField path="seed" type="integer | null">
  If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed.

  Default: `null`
</ParamField>

<ParamField path="stop" type="string | array | null">
  Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.

  Default: `null`
</ParamField>

<ParamField path="temperature" type="float | null">
  What sampling temperature to use, between 0 and 1.5. Higher values (e.g., 0.8) will make the output more random, while lower values (e.g., 0.2) will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.

  Default: `1.0`
</ParamField>

<ParamField path="top_p" type="float | null">
  An alternative to sampling with temperature, called nucleus sampling, where the model considers the tokens with top\_p probability mass. For example, 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.

  Default: `1.0`
</ParamField>

<ParamField path="echo" type="boolean">
  Echo back the prompt in addition to the completion. Incompatible with `return_raw_tokens=True`.

  Default: `false`
</ParamField>

<ParamField path="user" type="string | null">
  A unique identifier representing your end-user, which can help Cerebras to monitor and detect abuse.

  Default: `null`
</ParamField>

<ParamField path="logprobs" type="integer | null">
  Return log probabilities of the output tokens.

  For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.

  The max value is 20.

  Default: `null`

  <Note>
    Setting `logprobs` to 0 is different than `null`. When set to `null`, log probabilities are disabled entirely. When set to 0, log probabilities are enabled but it does not return `top_logprobs`.
  </Note>
</ParamField>

## Completion Response

<ParamField path="choices" type="object[]" required="true">
  The list of completion choices the model generated for the input prompt.

  <Expandable title="properties">
    <ParamField path="finish_reason" type="string | null">
      The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `content_filter` if content was omitted due to a flag from our content filters.
    </ParamField>

    <ParamField path="index" type="integer" />

    <ParamField path="logprobs" type="object | null" />

    <Expandable title="properties">
      <ParamField path="text_offset" type="array">
        Number of characters since the prompt.
      </ParamField>

      <ParamField path="token_logprobs" type="array">
        Logprob value for each token.
      </ParamField>

      <ParamField path="tokens" type="string">
        The tokens.
      </ParamField>

      <ParamField path="top_logprobs" type="array">
        List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
      </ParamField>
    </Expandable>

    <ParamField path="text" type="string" />
  </Expandable>
</ParamField>

<ParamField path="created" type="integer | null" required="true">
  The Unix timestamp (in seconds) of when the completion was created.
</ParamField>

<ParamField path="id" type="string">
  A unique identifier for the completion.
</ParamField>

<ParamField path="model" type="string">
  The model used for completion.
</ParamField>

<ParamField path="object" type="string" required="true">
  The object type, which is always "text\_completion"
</ParamField>

<ParamField path="system_fingerprint" type="string">
  This fingerprint represents the backend configuration that the model runs with.

  Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
</ParamField>

<ParamField path="usage" type="object">
  Usage statistics for the completion request.
</ParamField>

<RequestExample>
  ```python Python theme={null}
  import os
  from cerebras.cloud.sdk import Cerebras

  client = Cerebras(
      api_key=os.environ.get("CEREBRAS_API_KEY"),  # This is the default and can be omitted
  )

  completion = client.completions.create(
      prompt="It was a dark and stormy night",
      max_tokens=100,
      model="llama3.1-8b",
      logprobs=5,
  )

  print(completion)
  ```

  ```javascript Node.js theme={null}
  import Cerebras from '@cerebras/cerebras_cloud_sdk';

  const client = new Cerebras({
    apiKey: process.env['CEREBRAS_API_KEY'], // This is the default and can be omitted
  });

  async function main() {
    const completion = await client.completions.create({
      prompt: "It was a dark and stormy night",
      model: 'llama3.1-8b',
      logprobs: 5,
    });

    console.log(completion?.choices[0]?.text);
  }

  main();
  ```

  ```cli cURL theme={null}
  curl -X POST https://api.cerebras.ai/v1/completions \
     -H "Authorization: Bearer $CEREBRAS_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{
           "prompt": "It was a dark and stormy night",
           "max_tokens": 100,
           "model": "llama3.1-8b",
           "logprobs": 5
         }'
  ```
</RequestExample>

<ResponseExample>
  ```json Response theme={null}
  {
      "id": "chatcmpl-b8718798-d389-4421-9242-13b07e84983b",
      "choices": [
          {
              "finish_reason": "length",
              "index": 0,
              "text": " when I stumbled upon a small, quirky shop tucked away in a quiet alley. The sign above the door read \"Curios and Wonders,\" and the windows were filled with a dazzling array of strange and exotic items. I pushed open the door and stepped inside, my eyes adjusting to the dim light within.\n\nThe shop was a treasure trove of oddities, with shelves upon shelves of peculiar objects that seemed to defy explanation. There were vintage taxidermy animals, antique medical equipment, and",
               "logprobs": {
                "text_offset": [0, 5, 7, 16, 21, 23],
                "token_logprobs": [-0.15, -0.08, -0.22, -0.11, -0.19, -0.05],
                "tokens": [" when", " I", " stumbled", " upon", " a", " small"],
                "top_logprobs": [
                    {
                        " when": -0.15,
                        ",": -1.8,
                        ".": -2.3,
                        " and": -2.9,
                        " as": -3.2
                    },
                    {
                        " I": -0.08,
                        " the": -2.1,
                        " a": -2.7,
                        " she": -3.4,
                        " he": -3.6
                    },
                    {
                        " stumbled": -0.22,
                        " walked": -1.5,
                        " discovered": -2.8,
                        " came": -3.1,
                        " found": -3.5
                    }
                    // ... (remaining tokens omitted for brevity)
                 ]
             }
         }
      ],
      "created": 1731597024,
      "model": "llama3.1-8b",
      "system_fingerprint": "fp_e8eacef18a",
      "object": "text_completion",
      "usage": {
          "prompt_tokens": 10,
          "completion_tokens": 100,
          "total_tokens": 110
      },
      "time_info": {
          "queue_time": 4.673e-05,
          "prompt_time": 0.0004940576161616161,
          "completion_time": 0.045957338383838385,
          "total_time": 0.058876991271972656,
          "created": 1731597024
      }
  }
  ```
</ResponseExample>
